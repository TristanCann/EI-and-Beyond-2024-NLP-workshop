{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63d2af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c1365",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "Text data often needs preprocessing before analysis can take place. This can account for potential problems like inconsistencies in text formatting, typos and normalisation of punctuation and case.\n",
    "\n",
    "In the next cell, we introduce some example data for this workshop, a collection of online news articles from October 2021 that use either of the terms `climate change` or `global warming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e01f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804609e4",
   "metadata": {},
   "source": [
    "You can investigate the columns of this dataframe to see what metadata is available for these articles, but for the purpose of this workshop, we're most interested in the `title` and `body` columns. They contain the article headline and body text respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211e04df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trudeau says climate progress made at G20, though Canada wanted more ambitious plan - Haida Gwaii Observer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a5831",
   "metadata": {},
   "source": [
    "This example shows one of the cases where you may wish to apply preprocessing to a text, that is the addition of less informative boiler plate text. In this case it is the addition of the source name after the title. We can easily partition the text on a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47c6a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trudeau says climate progress made at G20, though Canada wanted more ambitious plan',\n",
       " 'Haida Gwaii Observer']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.title[0].split(' - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079479cc",
   "metadata": {},
   "source": [
    "A word of warning here - be careful if you apply such methods over larger volumes of text or from different sources as they may follow different linguistic styles.\n",
    "\n",
    "Another common cleaning step in NLP is to standardise the case in a text. Typically, NLP standardises to lowercase text. If this standardisation is not done, then the words `Biden`, `BIDEN`, and `biden` are considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5976f4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trudeau says climate progress made at g20, though canada wanted more ambitious plan - haida gwaii observer'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.title[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea33975",
   "metadata": {},
   "source": [
    "Another aspect of text cleaning you may wish to apply considers the punctuation present in a text. Removing all punctuation from a string is easy using the `string` library. You'll see in the example below that this is not a foolproof step - removal of `'` has contracted `Nicaragua's` to `Nicaraguas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba02ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling Catastrophe Risk to Protect against Natural Hazards: Nicaragua's Experience in Disaster Risk Management and Finance\n",
      "Pooling Catastrophe Risk to Protect against Natural Hazards Nicaraguas Experience in Disaster Risk Management and Finance\n"
     ]
    }
   ],
   "source": [
    "print(df_news.title[1])\n",
    "print(df_news.title[1].translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faef4de",
   "metadata": {},
   "source": [
    "The particular example listed above highlights another cleaning step you may wish to consider - stemming. Stemming is the process of identifying a word root from various derivative forms. For example, the stemmed forms of `fishes`, `fished` and `fishing` are all `fish`. As a result, you may find stemming a good way to group similar words. You'll see that in this case, the stemmer made all the text lower case automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32795aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pool',\n",
       " 'catastroph',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'against',\n",
       " 'natur',\n",
       " 'hazards:',\n",
       " \"nicaragua'\",\n",
       " 'experi',\n",
       " 'in',\n",
       " 'disast',\n",
       " 'risk',\n",
       " 'manag',\n",
       " 'and',\n",
       " 'financ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "[ps.stem(w) for w in df_news.title[1].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32123ed2",
   "metadata": {},
   "source": [
    "## A note on encoding\n",
    "\n",
    "Depending on the source of your text, you may find different text encodings, that is the way that the text is represented digitally. This is usually only problematic in the case of characters like emoji that are not included in the simplest alphabet. If you encounter such errors, try switching to a `utf-8` encoding when you read the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918093eb",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a couple of exercises to apply the techniques discussed above.\n",
    "\n",
    "First, count the number of unique words in the titles of all articles in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee92e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 45391 unique words in all titles.\n"
     ]
    }
   ],
   "source": [
    "## Extract all titles, and make a single long string from them.\n",
    "titles = ' '.join(df_news.title.values)\n",
    "## Remove all punctuation.\n",
    "titles = titles.translate(str.maketrans('','',string.punctuation))\n",
    "## Apply stemming to the words - note this is optional depending on your personal definition of a word.\n",
    "titles = [ps.stem(w) for w in titles.split()]\n",
    "## Converting to a set keeps only unique words, which we then count.\n",
    "print(f'There are {len(set(titles))} unique words in all titles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4842edde",
   "metadata": {},
   "source": [
    "Which is the most common form of the root word `talk` in the titles? Hint: you may find the `collections` library useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "37f03fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'talks': 2704, 'talk': 744, 'talking': 115, 'talked': 7})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "## Extract all titles, and make a single long string from them.\n",
    "titles = ' '.join(df_news.title.values)\n",
    "## Remove all punctuation.\n",
    "titles = titles.translate(str.maketrans('','',string.punctuation))\n",
    "titles = titles.lower().split()\n",
    "## Apply stemming to the words.\n",
    "titles_stemmed = [ps.stem(w) for w in titles]\n",
    "\n",
    "## Now we have lists of raw and stemmed words.\n",
    "talk_fulls = []\n",
    "for full, stem in zip(titles,titles_stemmed):\n",
    "    if stem == 'talk':\n",
    "            talk_fulls.append(full)\n",
    "talk_count = collections.Counter(talk_fulls)\n",
    "print(talk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b58022",
   "metadata": {},
   "source": [
    "# Text searching\n",
    "\n",
    "Cleaning the text is designed to make searching and matching parts of the text easier by removing issues that prevent an intended match.\n",
    "\n",
    "There are a few methods for matching texts in Python. First, we can work on strings directly. Notice the case sensitivity - this is why lower case conversion is important for some NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "72177a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trudeau says climate progress made at G20, though Canada wanted more ambitious plan - Haida Gwaii Observer\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df_news.title[0])\n",
    "print('Trudeau' in df_news.title[0])\n",
    "print('trudeau' in df_news.title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2726c6",
   "metadata": {},
   "source": [
    "A more powerful means of matching patterns in text uses `regular expressions`. They allow you to define patterns of text to be matched (e.g. any number followed by a bracket such as `1)`) without exhaustively listing the possible combinations. The full power of regex queries are too detailed for this workshop, but I recommend [regex101](https://regex101.com/) as a useful site to define and test queries.\n",
    "\n",
    "Here's an example query to find GXX groups in texts. Regex queries are normally defined by blocks to match within square brackets `[]` with certain special character groupings predefined. The example below looks for `G` followed by at least one (`+`) digit character (`\\d`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef0d6352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G20']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = r'[G][\\d]+'\n",
    "re.findall(query,df_news.title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbff46",
   "metadata": {},
   "source": [
    "Applying this over all titles we find the following matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "66202a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'G20': 3750, 'G7': 101, 'G5': 19, 'G100': 2, 'G3': 2, 'G1': 1, 'G2': 1, 'G02': 1, 'G77': 1})\n"
     ]
    }
   ],
   "source": [
    "g_groups = []\n",
    "for t in df_news.title:\n",
    "    g_groups += re.findall(query,t)\n",
    "print(collections.Counter(g_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93de6f",
   "metadata": {},
   "source": [
    "This example highlights an important caveat when defining text queries - check your data before and after returning the results. Here we were expecting the various levels of international economic fora like `G7` and `G20`. These weren't the only matches in our dataset - some of them are companies or other groups, but they are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1eda5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a couple of exercises on text matching. Note that these may be achievable with simple text matching, rather than regex but be aware of text inconsistencies.\n",
    "\n",
    "How many of the headlines mention Justin Trudeau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09ef9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 titles mention Trudeau.\n"
     ]
    }
   ],
   "source": [
    "## We'll consider a headline as relevant if it mentions just his surname.\n",
    "count = 0\n",
    "for t in df_news.title:\n",
    "    if 'trudeau' in t.lower():\n",
    "        count += 1\n",
    "print(f'{count} titles mention Trudeau.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d152eee",
   "metadata": {},
   "source": [
    "How many of the headlines mention one of the G7 countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a435a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13637 titles mention a G7 country.\n"
     ]
    }
   ],
   "source": [
    "## This query is not an exhaustive list - you could do more to match the UK and US.\n",
    "query = r'(Canada|Italy|France|Germany|Japan|United Kingdom|UK|United States|US|)'\n",
    "count = 0\n",
    "for t in df_news.title:\n",
    "    if np.any([m != '' for m in re.findall(query,t)]):\n",
    "        count += 1\n",
    "print(f'{count} titles mention a G7 country.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
