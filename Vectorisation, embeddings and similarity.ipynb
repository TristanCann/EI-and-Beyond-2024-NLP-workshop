{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b63317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62928997",
   "metadata": {},
   "source": [
    "# Text vectorisation\n",
    "\n",
    "For most computational analysis of text, you need to determine an appropriate way to represent your text as numbers. Typically this uses embedding, the concept of representing a piece of text as a vector that becomes a geometric representation of the meaning of a text. Here we are going to discuss a few different ways to embed texts and discuss their strengths and weaknesses.\n",
    "\n",
    "This overview uses a collection of online news articles from October 2021 that use either of the terms `climate change` or `global warming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d2de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6102f2",
   "metadata": {},
   "source": [
    "The simplest way to represent text as a vector is a `bag of words` approach. This approach defines a corpus of words, corresponding to the elements of a vector, and counts their frequency within a text.\n",
    "\n",
    "Let's look at this for the first five headlines. Here the corpus will be all words that appear in the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "527cb6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate', 'g20,', 'observer', 'trudeau', 'pooling', 'plan', 'risk', 'catastrophe', 'though', 'g-7', 'made', \"nicaragua's\", 'state', 'stettler', 'wanted', 'canada', 'in', 'to', 'at', 'against', '2021:', 'finance', 'management', 'progress', 'natural', 'impacts', 'analysis', 'hazards:', 'g-20', 'protect', 'plus?', '-', 'events', 'world', 'says', 'and', 'independent', 'of', 'experience', 'gwaii', 'major', 'minus', 'haida', 'ambitious', 'disaster', 'more', 'or', 'extreme']\n"
     ]
    }
   ],
   "source": [
    "headlines = [df_news.title[i].lower() for i in range(5)]\n",
    "corpus = list(set(' '.join(headlines).split()))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d92e46",
   "metadata": {},
   "source": [
    "There are also standard tools available to handle this through libraries like `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7df47098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '2021', 'against', 'ambitious', 'analysis', 'and', 'at', 'canada', 'catastrophe', 'climate', 'disaster', 'events', 'experience', 'extreme', 'finance', 'g20', 'gwaii', 'haida', 'hazards', 'impacts', 'in', 'independent', 'made', 'major', 'management', 'minus', 'more', 'natural', 'nicaragua', 'observer', 'of', 'or', 'plan', 'plus', 'pooling', 'progress', 'protect', 'risk', 'says', 'state', 'stettler', 'though', 'to', 'trudeau', 'wanted', 'world']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(headlines)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad35d6",
   "metadata": {},
   "source": [
    "Notice how we have slightly different terms in our corpus - this is because these standard libraries automatically include a range of preprocessing steps to streamline the text analysis process.\n",
    "\n",
    "Also, see how the corpus is already getting large - and you might realise that there are some words included that don't include much information such as `in`, `to`, `or`. We call these stopwords and have standard methods (and default lists) to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734a1c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '2021', 'ambitious', 'analysis', 'canada', 'catastrophe', 'climate', 'disaster', 'events', 'experience', 'extreme', 'finance', 'g20', 'gwaii', 'haida', 'hazards', 'impacts', 'independent', 'major', 'management', 'minus', 'natural', 'nicaragua', 'observer', 'plan', 'plus', 'pooling', 'progress', 'protect', 'risk', 'says', 'state', 'stettler', 'trudeau', 'wanted', 'world']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(headlines)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416c00da",
   "metadata": {},
   "source": [
    "We can also provide custom lists of stopwords - it makes sense to ignore `climate` since it features in our search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa6df84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '2021', 'ambitious', 'analysis', 'canada', 'catastrophe', 'disaster', 'events', 'experience', 'extreme', 'finance', 'g20', 'gwaii', 'haida', 'hazards', 'impacts', 'independent', 'major', 'management', 'minus', 'natural', 'nicaragua', 'observer', 'plan', 'plus', 'pooling', 'progress', 'protect', 'risk', 'says', 'state', 'stettler', 'trudeau', 'wanted', 'world']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import _stop_words\n",
    "stop = list(_stop_words.ENGLISH_STOP_WORDS) + ['climate']\n",
    "vectorizer = CountVectorizer(stop_words=stop)\n",
    "X = vectorizer.fit_transform(headlines)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1914b",
   "metadata": {},
   "source": [
    "Note that `sklearn` only provides English language stopwords, but other libraries have similar tools with a wider range of languages. Let's now look at the vector representations of the headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee01368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 2 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0]\n",
      " [0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())  ## The normal representation is more memory efficient, but this is easier for a human to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f435c",
   "metadata": {},
   "source": [
    "There are a few points to note about bag of words representations. Firstly, the representation is typically quite sparse, that is most entries in the array are `0`, which means that most headlines only use a few words in the corpus. It's also rare for any of the entries to be greater than `1` - this is a feature of choosing short headlines as our corpus. The last issue to consider is we lose all relationship between words besides appearing in the same headline - we don't know if words are consecutive or at either end of the text. This vectorisation does allow us to quickly compare texts for linguistic overlap however.\n",
    "\n",
    "The next embedding technique we're going to look at is called term frequency-inverse document frequency (`TF-IDF`). TF-IDF is a great way to compare texts and identify which words are more important to a text. It does this be normalising the term frequency calculated in the bag of words representation by the inverse document frequency ('IDF'). The IDF is a measure of how common a term is across all the text you're comparing. By adding this factor, we penalise terms that are common to many text as we expect their frequency to be naturally higher. We calculate this representation in a similar way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57615cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20', '2021', 'ambitious', 'analysis', 'canada', 'catastrophe', 'disaster', 'events', 'experience', 'extreme', 'finance', 'g20', 'gwaii', 'haida', 'hazards', 'impacts', 'independent', 'major', 'management', 'minus', 'natural', 'nicaragua', 'observer', 'plan', 'plus', 'pooling', 'progress', 'protect', 'risk', 'says', 'state', 'stettler', 'trudeau', 'wanted', 'world']\n",
      "[[0.         0.         0.28161876 0.         0.28161876 0.\n",
      "  0.         0.         0.         0.         0.         0.28161876\n",
      "  0.3490592  0.3490592  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.3490592  0.28161876\n",
      "  0.         0.         0.28161876 0.         0.         0.28161876\n",
      "  0.         0.         0.28161876 0.28161876 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.26726124\n",
      "  0.26726124 0.         0.26726124 0.         0.26726124 0.\n",
      "  0.         0.         0.26726124 0.         0.         0.\n",
      "  0.26726124 0.         0.26726124 0.26726124 0.         0.\n",
      "  0.         0.26726124 0.         0.26726124 0.53452248 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.30052135 0.         0.30052135 0.\n",
      "  0.         0.         0.         0.         0.         0.30052135\n",
      "  0.         0.         0.         0.         0.37248847 0.\n",
      "  0.         0.         0.         0.         0.         0.30052135\n",
      "  0.         0.         0.30052135 0.         0.         0.30052135\n",
      "  0.         0.37248847 0.30052135 0.30052135 0.        ]\n",
      " [0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.         0.         0.37796447 0.         0.37796447\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.         0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = TfidfVectorizer(stop_words=stop)\n",
    "X_tf = tf_vectorizer.fit_transform(headlines)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X_tf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1945a",
   "metadata": {},
   "source": [
    "While there is a limit to the conclusions that we can draw from such a small corpus, it does show the value of TF-IDF when comparing texts. Looking at the third row, we can see that each of its features are scored highly - this tells us that the words used in that title are consistently unique with reference to the other headlines we consider.\n",
    "\n",
    "So far, the embedding schemes we've considered have maintained a direct link between the words in the text and the values in the representation. This can become unmanageable with larger corpora. Let's look at an example with the first 1000 article bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c29bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 22652)\n"
     ]
    }
   ],
   "source": [
    "headlines = [df_news.body[i] for i in range(1000)]\n",
    "X_tf = tf_vectorizer.fit_transform(headlines)\n",
    "print(X_tf.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e3437",
   "metadata": {},
   "source": [
    "Now we have vectors of 22,652 numbers for each piece of text in our corpus - and this issue only increase when you consider longer texts. Vectors of this size can be very slow for a number of computational methods so we need to reduce them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb206b0d",
   "metadata": {},
   "source": [
    "# Text embeddings\n",
    "\n",
    "Text embeddings are the next step in this process. They take high-dimensional vector representations like we've seen in the previous cases and *embed* them in lower-dimensional spaces. These spaces are typically trained on large volumes of text and can capture meaning in the proximity of the vectors.\n",
    "\n",
    "There are a number of different pre-trained models for deriving these embeddings, here we'll look at the one of the leading tools: `BERT`. BERT was one of the first large language models to be made publicly available and was trained on an extensive corpus of text from across the internet. Since its release, there have been a number of updated and fine-tuned models made available that leverage the general understanding of BERT.\n",
    "\n",
    "Many of these models are included in the `sentence_transformers` library and detailed on [Hugging Face](https://huggingface.co/). Here, we'll demonstrate the smallest model, but by updating the model name it is easy to access any of the available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c3efec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b3e0c3f13b4bd8bbb04d854b2a36c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 384)\n"
     ]
    }
   ],
   "source": [
    "sentence_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = sentence_model.encode(df_news.title[:1000], show_progress_bar=True)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f734c9f",
   "metadata": {},
   "source": [
    "These embeddings are correspond to a 384-dimension vector space such that texts that are close in the vector space are close in meaning. Note that this 384 dimension space is much smaller than the 22,652 dimensions we found with TF-IDF, even if this model is a little slower to produce embeddings.\n",
    "\n",
    "There are a few pros and cons to be aware of with text embeddings. Since the model we're applying to the embeddings, adding new data is easy and only needs computation on the new data (compared to TF-IDF which needs the IDF values to be updated). In addition, the notion of semantic similarity in this space makes it easier to compare the meaning of texts. The drawbacks to embeddings come in the representation. Unlike the bag of words and TF-IDF representations, BERT embeddings are data dense and can therefore become large for significant corpora. The dimensions of the vector space are also removed from the underlying meaning of the words - they are instead *latent* dimensions that collect the relevance of many underlying words.\n",
    "\n",
    "One other caveat to note is that some BERT models have limits to the length of text that they accept as an input. You may not encounter this with your data, but be aware that splitting longer texts may give more reliable measures of the semantic information in the text - the examples here are *sentence transformers*, designed to work on shorter texts.\n",
    "\n",
    "The notion of semantic similarity in the text embeddings means that we can determine if two texts have similar meaning by taking the cosine similarity of their embedding vectors. This measures their proximity in the embedding space - which is trained such that texts with similar meanings should be close. Identical texts will have a cosine similarity of 1, whereas dissimilar texts will have cosine similarity approaching 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a26a4fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f910cf31c486ababaa401545483e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000001  0.5976263  0.13372496]\n",
      " [0.5976263  1.0000004  0.03286065]\n",
      " [0.13372496 0.03286065 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sents = ['I took a walk down the street.','I strode along the promenade.','The cat ate its supper.']\n",
    "embs = sentence_model.encode(sents,show_progress_bar=True)\n",
    "print(cosine_similarity(embs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518a2e7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are some exercises to practice deriving a comparing representations of texts within a corpus.\n",
    "\n",
    "Find the five most similar headline among the first 1000 headlines to the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b423601b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df1e1cf87ca498ebff08ebad27e6e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absence of key world leaders hangs over Biden's first G-20 0.47943187\n",
      "Biden reflects on American leadership, progress made at G-20 summit 0.48371822\n",
      "Biden reflects on American leadership, progress made at G-20 summit 0.48371822\n",
      "Biden reflects on American leadership, progress made at G-20 summit 0.48371822\n",
      "Biden reflects on American leadership, progress made at G-20 summit 0.48371822\n"
     ]
    }
   ],
   "source": [
    "ref_text = ['President Biden visited other G7 leaders in London']\n",
    "sents = ref_text + [t for t in df_news.title[:1000]]\n",
    "embs = sentence_model.encode(sents,show_progress_bar=True)\n",
    "\n",
    "## We can calculate pairwise similarity easily.\n",
    "sim = cosine_similarity(embs)\n",
    "## The first row gives the similarity scores we care about. Find the maximal elements.\n",
    "matches = np.argsort(sim[0])\n",
    "for i in matches[-6:-1]:\n",
    "    print(sents[i],sim[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e35d0",
   "metadata": {},
   "source": [
    "Which of the first 1000 articles uses *climate* the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b92c32ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 576th article uses climate 159 times\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_1k = vectorizer.fit_transform([t for t in df_news.body[:1000]])\n",
    "\n",
    "## We need to get the feature names from vectorizer to find out which column of X_1k to check.\n",
    "clim_col = list(vectorizer.get_feature_names_out()).index('climate')\n",
    "\n",
    "max_doc = np.argmax(X_1k[:,clim_col])\n",
    "print(f'The {max_doc}th article uses climate {X_1k[max_doc,clim_col]} times')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
